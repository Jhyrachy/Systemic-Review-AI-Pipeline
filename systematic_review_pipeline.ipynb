{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3661071f",
   "metadata": {},
   "source": [
    "# üî¨ Systematic Review Pipeline\n",
    "\n",
    "**Complete AI-powered automation for literature search, deduplication, and abstract screening.**\n",
    "\n",
    "**üìñ Complete documentation**: See `README.md` for detailed setup, configuration options, troubleshooting, and best practices.\n",
    "\n",
    "## Quick Overview\n",
    "- **üß† AI Query Generation**: From protocol documents\n",
    "- **üîç PubMed Integration**: Automated literature search\n",
    "- **üßπ Smart Deduplication**: Cross-database duplicate removal  \n",
    "- **üéØ AI Screening**: Professional abstract evaluation\n",
    "- **üìä Excel Exports**: Comprehensive reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb83663",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Required Libraries\n",
    "Initialize all necessary Python libraries for data processing, API calls, and file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6703b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Session: 20251016_232310\n",
      "üíæ Cache: cache/session_20251016_232310/\n",
      "üìä Export: export_results/session_20251016_232310/\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "\n",
    "# Session organization: separate cache and export directories\n",
    "SESSION_ID = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "CACHE_DIR = f\"cache/session_{SESSION_ID}\"\n",
    "EXPORT_DIR = f\"export_results/session_{SESSION_ID}\"\n",
    "\n",
    "# Create both directories\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Session: {SESSION_ID}\")\n",
    "print(f\"üíæ Cache: {CACHE_DIR}/\")\n",
    "print(f\"üìä Export: {EXPORT_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083c10b",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: LLM Configuration & API Setup\n",
    "Configure AI models for query generation and screening. Verify API key connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576de9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checking API configuration...\n",
      "‚úÖ API key found: sk-or-v1...fb30\n",
      "ü§ñ LLM ready!\n",
      "   Model: google/gemini-2.5-flash\n",
      "   Query Model: google/gemini-2.5-pro\n"
     ]
    }
   ],
   "source": [
    "# LLM Configuration\n",
    "LLM_CONFIG = {\n",
    "    'model': 'google/gemini-2.5-flash',\n",
    "    'query_model': 'google/gemini-2.5-pro',  # For protocol analysis\n",
    "    'api_base': 'https://openrouter.ai/api/v1',\n",
    "    'temperature': 0.1,\n",
    "    'max_tokens': 10000\n",
    "}\n",
    "\n",
    "def call_llm(prompt, model=None, temperature=None):\n",
    "    \"\"\"Call LLM with prompt.\"\"\"\n",
    "    # Get API key\n",
    "    api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"‚ùå OPENROUTER_API_KEY not found!\")\n",
    "        print(\"   Set it with: export OPENROUTER_API_KEY='your-key'\")\n",
    "        return None\n",
    "    \n",
    "    # Setup parameters\n",
    "    model = model or LLM_CONFIG['model']\n",
    "    temperature = temperature or LLM_CONFIG['temperature']\n",
    "    \n",
    "    try:\n",
    "        # Create client and make request\n",
    "        client = OpenAI(api_key=api_key, base_url=LLM_CONFIG['api_base'])\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=LLM_CONFIG['max_tokens'],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test API key\n",
    "print(\" Checking API configuration...\")\n",
    "api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if api_key:\n",
    "    print(f\"‚úÖ API key found: {api_key[:8]}...{api_key[-4:]}\")\n",
    "    print(f\"ü§ñ LLM ready!\")\n",
    "    print(f\"   Model: {LLM_CONFIG['model']}\")\n",
    "    print(f\"   Query Model: {LLM_CONFIG['query_model']}\")\n",
    "else:\n",
    "    print(\"‚ùå OPENROUTER_API_KEY not set!\")\n",
    "    print(\"   Set it with: export OPENROUTER_API_KEY='your-key'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a49ec1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Search Query Configuration\n",
    "\n",
    "**AI Mode**: Leave `USER_DEFINED_QUERY` empty + place `protocol.docx` in directory  \n",
    "**Manual Mode**: Set your PubMed query in `USER_DEFINED_QUERY` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333cb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Query Configuration\n",
    "# Set your own PubMed search query here if you don't want to use AI generation\n",
    "\n",
    "# Option 1: Leave empty to use AI generation\n",
    "USER_DEFINED_QUERY = \"\"\n",
    "\n",
    "# Option 2: Set your own query (examples below)\n",
    "# USER_DEFINED_QUERY = '(\"penile cancer\"[tiab] OR \"penile carcinoma\"[tiab]) AND (\"lymphadenectomy\"[tiab] OR \"lymph node dissection\"[tiab])'\n",
    "# USER_DEFINED_QUERY = '(\"diabetes\"[MeSH Terms] OR \"diabetes mellitus\"[tiab]) AND (\"exercise\"[MeSH Terms] OR \"physical activity\"[tiab]) AND (\"randomized controlled trial\"[pt] OR \"clinical trial\"[pt])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622f3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù No manual query set - AI generation will be used if protocol.docx is available\n"
     ]
    }
   ],
   "source": [
    "# Study configuration for manual queries\n",
    "MANUAL_STUDY_CONFIG = {\n",
    "    'study_title': 'Manual Query Systematic Review',\n",
    "    'research_topic': 'User-defined search',\n",
    "    'primary_outcome': 'As defined by user query',\n",
    "    'existing_database_file': 'database.xlsx',\n",
    "    'protocol_file': 'protocol.docx',\n",
    "    'output_prefix': 'ai_search_manual'\n",
    "}\n",
    "\n",
    "if USER_DEFINED_QUERY.strip():\n",
    "    print(\"‚úÖ Manual query detected!\")\n",
    "    print(f\"üîç Query: {USER_DEFINED_QUERY}\")\n",
    "    print(\"ü§ñ AI generation will be skipped\")\n",
    "    \n",
    "    # Set global variables for manual mode\n",
    "    SEARCH_QUERY = USER_DEFINED_QUERY.strip()\n",
    "    STUDY_CONFIG = MANUAL_STUDY_CONFIG\n",
    "    AI_MODE = False\n",
    "    \n",
    "else:\n",
    "    print(\"üìù No manual query set - AI generation will be used if protocol.docx is available\")\n",
    "    SEARCH_QUERY = None\n",
    "    STUDY_CONFIG = None\n",
    "    AI_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a7d4b",
   "metadata": {},
   "source": [
    "## üß† Step 4: AI Query Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c015716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AI Query Generation ready!\n",
      "   üìÑ extract_text_from_protocol()\n",
      "   üéØ generate_pubmed_query_from_protocol()\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_protocol(file_path):\n",
    "    \"\"\"Extract text from protocol document (.docx, .txt, or .pdf).\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.docx'):\n",
    "            doc = Document(file_path)\n",
    "            return '\\n'.join([p.text for p in doc.paragraphs])\n",
    "        \n",
    "        elif file_path.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        \n",
    "        elif file_path.endswith('.pdf'):\n",
    "            try:\n",
    "                import PyPDF2\n",
    "                text = []\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    pdf_reader = PyPDF2.PdfReader(f)\n",
    "                    for page in pdf_reader.pages:\n",
    "                        text.append(page.extract_text())\n",
    "                return '\\n'.join(text)\n",
    "            except ImportError:\n",
    "                print(\"‚ö†Ô∏è  PyPDF2 not installed. Install with: pip install PyPDF2\")\n",
    "                return None\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Unsupported format: {file_path}\")\n",
    "            print(f\"   Supported: .docx, .txt, .pdf\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_query_from_response(response_text):\n",
    "    \"\"\"Extract query from AI response.\"\"\"\n",
    "    \n",
    "    # Look for QUERY_START...QUERY_END\n",
    "    pattern = r'QUERY_START\\s*(.*?)\\s*QUERY_END'\n",
    "    match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        query = match.group(1).strip()\n",
    "        query = re.sub(r'\\s+', ' ', query)  # Clean whitespace\n",
    "        return query\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è  Could not extract query\")\n",
    "    return None\n",
    "\n",
    "def generate_pubmed_query_from_protocol(protocol_file_path, model=None):\n",
    "    \"\"\"Generate PubMed search query from protocol using AI.\"\"\"\n",
    "    print(f\"üß† Generating query from: {protocol_file_path}\")\n",
    "    \n",
    "    protocol_text = extract_text_from_protocol(protocol_file_path)\n",
    "    if not protocol_text:\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÑ Protocol loaded: {len(protocol_text)} chars\")\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert systematic review methodologist specializing in PubMed search strategy development.\n",
    "\n",
    "TASK: Analyze the protocol below and create a comprehensive, sensitive PubMed search query.\n",
    "\n",
    "SEARCH STRATEGY REQUIREMENTS:\n",
    "1. Apply PICO framework (Population, Intervention, Comparator, Outcome)\n",
    "2. For each concept:\n",
    "   - Include MeSH terms: [MeSH Terms]\n",
    "   - Include text words: [tiab] (title/abstract)\n",
    "   - Add synonyms and related terms\n",
    "   - Use wildcards (*) for word variations (e.g., diagnos* for diagnosis/diagnostic)\n",
    "3. Combine terms within concepts with OR\n",
    "4. Combine concepts with AND\n",
    "5. Balance sensitivity (finding all relevant) with specificity (avoiding irrelevant)\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT add date restrictions unless specified in protocol\n",
    "- Do NOT add language filters unless specified\n",
    "- Include both specific terms and broader concepts\n",
    "- Consider spelling variations and abbreviations\n",
    "\n",
    "OUTPUT FORMAT (mandatory):\n",
    "\n",
    "QUERY_START\n",
    "[Your complete PubMed query here - must be valid PubMed syntax]\n",
    "QUERY_END\n",
    "\n",
    "EXAMPLE:\n",
    "QUERY_START\n",
    "(\"diabetes mellitus\"[MeSH Terms] OR \"diabetes mellitus, type 2\"[MeSH Terms] OR diabetes[tiab] OR diabetic[tiab]) AND (\"exercise\"[MeSH Terms] OR \"physical activity\"[tiab] OR \"aerobic exercise\"[tiab] OR training[tiab]) AND (\"glycated hemoglobin\"[MeSH Terms] OR \"hemoglobin a1c\"[tiab] OR hba1c[tiab])\n",
    "QUERY_END\n",
    "\n",
    "---\n",
    "\n",
    "PROTOCOL TO ANALYZE:\n",
    "{protocol_text}\n",
    "\n",
    "Generate the optimized PubMed search query now:\"\"\"\n",
    "\n",
    "    response = call_llm(\n",
    "        prompt=prompt,\n",
    "        model=LLM_CONFIG['query_model'],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    if not response:\n",
    "        print(\"‚ùå Failed to generate query\")\n",
    "        return None\n",
    "\n",
    "    query = extract_query_from_response(response)\n",
    "    \n",
    "    if query:\n",
    "        print(\"‚úÖ Query generated!\")\n",
    "        print(f\"üîó {query}\")\n",
    "        return query\n",
    "    else:\n",
    "        print(\"‚ùå Extraction failed\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ AI Query Generation ready!\")\n",
    "print(\"   üìÑ extract_text_from_protocol()\")\n",
    "print(\"   üéØ generate_pubmed_query_from_protocol()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadeed93",
   "metadata": {},
   "source": [
    "## üîç Step 5: Query Generation Execution\n",
    "Determine search strategy based on configuration and generate final PubMed query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a6bbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query Generation...\n",
      "üìÇ Loaded from cache: ((\"Penile Neoplasms\"[MeSH] OR \"penile cancer\"[tiab] OR \"penile carcinoma\"[tiab] OR \"cancer of the pe...\n",
      "üöÄ Ready to search!\n"
     ]
    }
   ],
   "source": [
    "# Query Generation Workflow\n",
    "print(\"üîç Query Generation...\")\n",
    "\n",
    "if not AI_MODE:\n",
    "    # Manual query mode\n",
    "    print(f\"‚úÖ Using manual query: {SEARCH_QUERY[:100]}...\")\n",
    "    \n",
    "elif os.path.exists('protocol.docx'):\n",
    "    # AI mode - check cache first\n",
    "    query_cache = f\"{CACHE_DIR}/generated_query.txt\"\n",
    "    \n",
    "    if os.path.exists(query_cache):\n",
    "        with open(query_cache, 'r', encoding='utf-8') as f:\n",
    "            SEARCH_QUERY = f.read().strip()\n",
    "        print(f\"üìÇ Loaded from cache: {SEARCH_QUERY[:100]}...\")\n",
    "    else:\n",
    "        # Generate new query\n",
    "        SEARCH_QUERY = generate_pubmed_query_from_protocol('protocol.docx')\n",
    "        if SEARCH_QUERY:\n",
    "            with open(query_cache, 'w', encoding='utf-8') as f:\n",
    "                f.write(SEARCH_QUERY)\n",
    "            print(f\"üíæ Cached to: {query_cache}\")\n",
    "        else:\n",
    "            print(\"‚ùå Query generation failed - check API key\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå No manual query and no protocol.docx found\")\n",
    "    print(\"   Set USER_DEFINED_QUERY or add protocol.docx\")\n",
    "    SEARCH_QUERY = None\n",
    "\n",
    "if SEARCH_QUERY:\n",
    "    print(\"üöÄ Ready to search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c63041",
   "metadata": {},
   "source": [
    "## üíæ Step 6: Caching System Setup\n",
    "Configure intelligent caching to avoid redundant API calls and speed up repeated searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf2bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Caching system ready!\n"
     ]
    }
   ],
   "source": [
    "# Caching configuration\n",
    "CACHE_CONFIG = {\n",
    "    'cache_dir': CACHE_DIR,  # Use session cache directory\n",
    "    'cache_duration_hours': 24,\n",
    "    'user_email': 'researcher@example.com',\n",
    "    'max_results': 1000, # Max results to fetch from PubMed\n",
    "    'api_delay': 0.5\n",
    "}\n",
    "\n",
    "os.makedirs(CACHE_CONFIG['cache_dir'], exist_ok=True)\n",
    "\n",
    "def get_cache_filename(query):\n",
    "    \"\"\"Generate cache filename based on query hash.\"\"\"\n",
    "    import hashlib\n",
    "    query_hash = hashlib.md5(query.encode()).hexdigest()[:8]\n",
    "    return f\"{CACHE_CONFIG['cache_dir']}/pubmed_search_{query_hash}.json\"\n",
    "\n",
    "def is_cache_valid(cache_file):\n",
    "    \"\"\"Check if cache file exists and is still valid.\"\"\"\n",
    "    if not os.path.exists(cache_file):\n",
    "        return False\n",
    "    cache_age = datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_file))\n",
    "    return cache_age.total_seconds() < CACHE_CONFIG['cache_duration_hours'] * 3600\n",
    "\n",
    "def save_to_cache(data, cache_file):\n",
    "    \"\"\"Save data to cache file.\"\"\"\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'timestamp': datetime.now().isoformat(), 'data': data}, f, indent=2)\n",
    "    print(f\"üíæ Data cached to: {cache_file}\")\n",
    "\n",
    "def load_from_cache(cache_file):\n",
    "    \"\"\"Load data from cache file.\"\"\"\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        cache_data = json.load(f)\n",
    "    cached_time = datetime.fromisoformat(cache_data['timestamp'])\n",
    "    print(f\"üìÇ Using cached data from: {cached_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    return cache_data['data']\n",
    "\n",
    "print(\"üîÑ Caching system ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d2af4",
   "metadata": {},
   "source": [
    "## üåê Step 7: PubMed Search Functions\n",
    "\n",
    "Execute the search and retrieve study details from PubMed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7454d7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç PubMed search functions ready!\n"
     ]
    }
   ],
   "source": [
    "def search_pubmed_ids(query, max_results=10000, email=\"researcher@example.com\"):\n",
    "    \"\"\"Search PubMed and return list of PMIDs.\"\"\"\n",
    "    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': query,\n",
    "        'retmax': max_results,\n",
    "        'retmode': 'xml',\n",
    "        'email': email,\n",
    "        'tool': 'systematic_review_pipeline'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(search_url, params=params, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    root = ET.fromstring(response.content)\n",
    "    return [id_elem.text for id_elem in root.findall('.//Id')]\n",
    "\n",
    "def fetch_study_details(pmids, email=\"researcher@example.com\", batch_size=200):\n",
    "    \"\"\"Fetch detailed information for list of PMIDs in batches.\"\"\"\n",
    "    all_studies = []\n",
    "    \n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch_pmids = pmids[i:i + batch_size]\n",
    "        print(f\"   üì• Fetching batch {i//batch_size + 1}/{(len(pmids)-1)//batch_size + 1}\")\n",
    "        \n",
    "        fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "        params = {\n",
    "            'db': 'pubmed',\n",
    "            'id': ','.join(batch_pmids),\n",
    "            'rettype': 'xml',\n",
    "            'retmode': 'xml',\n",
    "            'email': email,\n",
    "            'tool': 'systematic_review_pipeline'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(fetch_url, params=params, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        root = ET.fromstring(response.content)\n",
    "        for article in root.findall('.//PubmedArticle'):\n",
    "            study_data = parse_article(article)\n",
    "            if study_data:\n",
    "                all_studies.append(study_data)\n",
    "        \n",
    "        time.sleep(CACHE_CONFIG['api_delay'])\n",
    "    \n",
    "    return all_studies\n",
    "\n",
    "def get_text(element, path, default=''):\n",
    "    \"\"\"Helper to safely extract text from XML element.\"\"\"\n",
    "    elem = element.find(path)\n",
    "    return elem.text if elem is not None and elem.text else default\n",
    "\n",
    "def parse_article(article_xml):\n",
    "    \"\"\"Parse PubMed article XML into dictionary.\"\"\"\n",
    "    # Basic fields\n",
    "    data = {\n",
    "        'PMID': get_text(article_xml, './/PMID'),\n",
    "        'Title': get_text(article_xml, './/ArticleTitle'),\n",
    "        'Journal/Book': get_text(article_xml, './/Journal/Title') or get_text(article_xml, './/Journal/ISOAbbreviation'),\n",
    "        'Create Date': datetime.now().strftime('%Y-%m-%d')\n",
    "    }\n",
    "    \n",
    "    # Abstract (pu√≤ essere strutturato con label)\n",
    "    abstract_parts = []\n",
    "    for abs_elem in article_xml.findall('.//AbstractText'):\n",
    "        if abs_elem.text:\n",
    "            label = abs_elem.get('Label', '')\n",
    "            if label and label.upper() not in ['UNLABELLED', 'UNASSIGNED']:\n",
    "                abstract_parts.append(f\"{label}: {abs_elem.text}\")\n",
    "            else:\n",
    "                abstract_parts.append(abs_elem.text)\n",
    "    data['Abstract'] = ' '.join(abstract_parts)\n",
    "    \n",
    "    # Authors\n",
    "    authors = []\n",
    "    for author in article_xml.findall('.//Author'):\n",
    "        last = get_text(author, 'LastName')\n",
    "        first = get_text(author, 'ForeName')\n",
    "        if last:\n",
    "            authors.append(f\"{last}, {first}\" if first else last)\n",
    "    data['Authors'] = '; '.join(authors)\n",
    "    \n",
    "    # Publication Year\n",
    "    year = get_text(article_xml, './/PubDate/Year')\n",
    "    if not year:\n",
    "        medline_date = get_text(article_xml, './/PubDate/MedlineDate')\n",
    "        if medline_date:\n",
    "            match = re.search(r'\\d{4}', medline_date)\n",
    "            year = match.group() if match else ''\n",
    "    data['Publication Year'] = year\n",
    "    \n",
    "    # DOI e PMCID\n",
    "    doi_elem = article_xml.find('.//ArticleId[@IdType=\"doi\"]')\n",
    "    data['DOI'] = doi_elem.text if doi_elem is not None else ''\n",
    "    \n",
    "    pmc_elem = article_xml.find('.//ArticleId[@IdType=\"pmc\"]')\n",
    "    data['PMCID'] = pmc_elem.text if pmc_elem is not None else ''\n",
    "    \n",
    "    # Citation\n",
    "    if data['Authors'] and data['Title'] and data['Journal/Book']:\n",
    "        data['Citation'] = f\"{data['Authors']}. {data['Title']} {data['Journal/Book']}. {data['Publication Year']}.\"\n",
    "    else:\n",
    "        data['Citation'] = ''\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"üîç PubMed search functions ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49707e",
   "metadata": {},
   "source": [
    "## üöÄ Step 8: Execute PubMed Search\n",
    "Perform the literature search and retrieve studies based on the configured query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610685d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting PubMed search...\n",
      "üìù Query: ((\"Penile Neoplasms\"[MeSH] OR \"penile cancer\"[tiab] OR \"penile carcinoma\"[tiab] OR \"cancer of the penis\"[tiab] OR \"penile tumo*r\"[tiab] OR \"penile malignan*\"[tiab])) AND ((\"Lymph Node Excision\"[MeSH] OR lymphadenectom*[tiab] OR \"lymph node dissection*\"[tiab] OR \"lymph node excision\"[tiab] OR \"nodal dissection*\"[tiab] OR \"groin dissection*\"[tiab] OR \"inguinal dissection*\"[tiab] OR \"pelvic dissection*\"[tiab] OR \"inguinal lymphadenectomy\"[tiab] OR \"pelvic lymphadenectomy\"[tiab])) AND ((\"Neoplasm Recurrence, Local\"[MeSH] OR \"Recurrence\"[MeSH] OR recurren*[tiab] OR relapse*[tiab] OR \"local recurrence\"[tiab] OR \"regional recurrence\"[tiab] OR \"distant recurrence\"[tiab])) AND ((\"Prognosis\"[MeSH] OR \"Risk Factors\"[MeSH] OR \"Survival Analysis\"[MeSH] OR \"Predictive Value of Tests\"[MeSH] OR prognos*[tiab] OR predict*[tiab] OR \"risk factor*\"[tiab] OR \"risk stratification\"[tiab] OR determinant*[tiab] OR associat*[tiab] OR clinicopathologic*[tiab] OR clinicopathological[tiab]))\n",
      "======================================================================\n",
      "üåê No valid cache found. Fetching from PubMed API...\n",
      "\n",
      "üìã Step 1: Searching for PMIDs...\n",
      "‚úÖ Found 5 studies\n",
      "\n",
      "üìö Step 2: Fetching detailed information for 5 studies...\n",
      "   üì• Fetching batch 1/1\n",
      "üíæ Data cached to: cache/session_20251016_232310/pubmed_search_31f6339e.json\n",
      "\n",
      "‚úÖ Successfully retrieved 5 studies\n",
      "\n",
      "======================================================================\n",
      "üìä RESULTS SUMMARY\n",
      "======================================================================\n",
      "Total studies retrieved: 5\n",
      "\n",
      "Sample of retrieved studies:\n",
      "       PMID                                              Title  \\\n",
      "0  41008879  Comparing the Perioperative and Oncological Ou...   \n",
      "1  40671095  Performing sentinel lymph node biopsy without ...   \n",
      "2  40669117  Association of tumor budding with prognostic f...   \n",
      "3  40379592  Evaluating the role of adjuvant therapy in imp...   \n",
      "4  40079925  Novel Role Of DSNB in Staging of Primary Ureth...   \n",
      "\n",
      "  Publication Year                                       Journal/Book  \n",
      "0             2025                                            Cancers  \n",
      "1             2025                 World journal of surgical oncology  \n",
      "2             2025                     Annals of diagnostic pathology  \n",
      "3             2025                                  Urologic oncology  \n",
      "4             2025  International braz j urol : official journal o...  \n",
      "\n",
      "‚úÖ PubMed search completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute PubMed search with caching\n",
    "print(\"üîç Starting PubMed search...\")\n",
    "print(f\"üìù Query: {SEARCH_QUERY}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check cache first\n",
    "cache_file = get_cache_filename(SEARCH_QUERY)\n",
    "\n",
    "if is_cache_valid(cache_file):\n",
    "    print(\"‚úÖ Valid cache found! Loading from cache...\")\n",
    "    all_studies = load_from_cache(cache_file)\n",
    "    studies_df = pd.DataFrame(all_studies)\n",
    "else:\n",
    "    print(\"üåê No valid cache found. Fetching from PubMed API...\")\n",
    "    \n",
    "    # Step 1: Search for PMIDs\n",
    "    print(\"\\nüìã Step 1: Searching for PMIDs...\")\n",
    "    pmids = search_pubmed_ids(\n",
    "        query=SEARCH_QUERY,\n",
    "        max_results=CACHE_CONFIG['max_results'],\n",
    "        email=CACHE_CONFIG['user_email']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(pmids)} studies\")\n",
    "    \n",
    "    if not pmids:\n",
    "        print(\"‚ùå No studies found. Please check your query.\")\n",
    "        studies_df = pd.DataFrame()\n",
    "    else:\n",
    "        # Step 2: Fetch detailed information\n",
    "        print(f\"\\nüìö Step 2: Fetching detailed information for {len(pmids)} studies...\")\n",
    "        all_studies = fetch_study_details(\n",
    "            pmids=pmids,\n",
    "            email=CACHE_CONFIG['user_email'],\n",
    "            batch_size=200\n",
    "        )\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        studies_df = pd.DataFrame(all_studies)\n",
    "        \n",
    "        # Save to cache\n",
    "        save_to_cache(all_studies, cache_file)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully retrieved {len(studies_df)} studies\")\n",
    "\n",
    "# Display results summary\n",
    "if not studies_df.empty:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä RESULTS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total studies retrieved: {len(studies_df)}\")\n",
    "    print(f\"\\nSample of retrieved studies:\")\n",
    "    print(studies_df[['PMID', 'Title', 'Publication Year', 'Journal/Book']].head())\n",
    "    print(\"\\n‚úÖ PubMed search completed successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No studies to display\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40f9dd",
   "metadata": {},
   "source": [
    "## üßπ Step 9: Simple Deduplication Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1637b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Deduplication ready! Simple PMID/DOI/title matching in all cells\n"
     ]
    }
   ],
   "source": [
    "def load_existing_databases(database_folder=\"old_database\"):\n",
    "    \"\"\"Load and clean all database files from folder.\"\"\"\n",
    "    if not os.path.exists(database_folder):\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    files = [os.path.join(database_folder, f) for f in os.listdir(database_folder) \n",
    "             if f.endswith(('.xlsx', '.csv', '.xls'))]\n",
    "    \n",
    "    if not files:\n",
    "        return pd.DataFrame(), []\n",
    "    \n",
    "    print(f\"üìö Loading {len(files)} database(s):\")\n",
    "    studies = []\n",
    "    \n",
    "    for db_file in files:\n",
    "        print(f\"   üìñ {os.path.basename(db_file)}\")\n",
    "        \n",
    "        if db_file.endswith('.csv'):\n",
    "            df = pd.read_csv(db_file).dropna(how='all').drop_duplicates()\n",
    "            df['Source_Database'] = os.path.basename(db_file)\n",
    "            studies.append(df)\n",
    "        else:\n",
    "            for sheet in pd.ExcelFile(db_file).sheet_names:\n",
    "                df = pd.read_excel(db_file, sheet_name=sheet)\n",
    "                orig, df = len(df), df.dropna(how='all').drop_duplicates()\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    df['Source_Database'] = f\"{os.path.basename(db_file)}:{sheet}\"\n",
    "                    studies.append(df)\n",
    "                    removed = orig - len(df)\n",
    "                    print(f\"      üìÑ {sheet}: {len(df)} studies\" + (f\" (-{removed})\" if removed else \"\"))\n",
    "    \n",
    "    if studies:\n",
    "        combined = pd.concat(studies, ignore_index=True)\n",
    "        print(f\"‚úÖ Total: {len(combined)} studies\")\n",
    "        return combined, files\n",
    "    \n",
    "    return pd.DataFrame(), files\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize: lowercase, no punctuation, no extra spaces.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    import string\n",
    "    return ' '.join(str(text).lower().translate(str.maketrans('', '', string.punctuation)).split())\n",
    "\n",
    "def check_duplicates(new_df, existing_df):\n",
    "    \"\"\"Find duplicates by searching PMID/DOI/title in existing database.\"\"\"\n",
    "    if existing_df.empty:\n",
    "        return []\n",
    "    \n",
    "    print(f\"üîç Checking {len(new_df)} new vs {len(existing_df)} existing...\")\n",
    "    \n",
    "    # Build searchable set\n",
    "    values = set()\n",
    "    for col in existing_df.columns:\n",
    "        for v in existing_df[col].dropna().astype(str):\n",
    "            v = v.strip()\n",
    "            if v:\n",
    "                values.add(v)\n",
    "                if len(v) > 20:  # Likely title\n",
    "                    norm = normalize_text(v)\n",
    "                    if norm:\n",
    "                        values.add(norm)\n",
    "    \n",
    "    print(f\"   üìä {len(values)} searchable values\")\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = set()\n",
    "    for idx, study in new_df.iterrows():\n",
    "        for field in ['PMID', 'DOI', 'Title']:\n",
    "            if pd.notna(study.get(field, '')):\n",
    "                val = str(study[field]).strip()\n",
    "                check_val = normalize_text(val) if field == 'Title' else val\n",
    "                \n",
    "                if val in values or (check_val and check_val in values):\n",
    "                    duplicates.add(idx)\n",
    "                    print(f\"   üîÑ {field}: {val[:50]}...\")\n",
    "                    break\n",
    "    \n",
    "    print(f\"üìä {len(duplicates)} duplicates found\")\n",
    "    return list(duplicates)\n",
    "\n",
    "def deduplicate_studies(new_df, database_folder=\"old_database\"):\n",
    "    \"\"\"Load databases and remove duplicates.\"\"\"\n",
    "    print(\"üßπ Deduplication...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    existing_df, files = load_existing_databases(database_folder)\n",
    "    \n",
    "    stats = {'total_new': len(new_df), 'database_files': files}\n",
    "    \n",
    "    if existing_df.empty:\n",
    "        print(\"‚úÖ No existing databases - all new\")\n",
    "        stats.update({'duplicates_found': 0, 'unique_studies': len(new_df)})\n",
    "        return new_df, stats\n",
    "    \n",
    "    dup_idx = check_duplicates(new_df, existing_df)\n",
    "    \n",
    "    if dup_idx:\n",
    "        result = new_df.drop(dup_idx).reset_index(drop=True)\n",
    "        print(f\"\\n‚úÖ Summary: {len(new_df)} retrieved, {len(dup_idx)} removed, {len(result)} unique\")\n",
    "    else:\n",
    "        result = new_df\n",
    "        print(f\"\\n‚úÖ All {len(new_df)} studies are unique!\")\n",
    "    \n",
    "    stats.update({'duplicates_found': len(dup_idx), 'unique_studies': len(result)})\n",
    "    return result, stats\n",
    "\n",
    "print(\"üßπ Deduplication ready! Simple PMID/DOI/title matching in all cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aad65a",
   "metadata": {},
   "source": [
    "## ‚ú® Step 10: Execute Deduplication\n",
    "\n",
    "**Process**: Compare against databases in `old_database/` folder using PMID, DOI, and title matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e2535ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Starting deduplication process...\n",
      "==================================================\n",
      "üßπ Deduplication...\n",
      "==================================================\n",
      "üìö Loading 1 database(s):\n",
      "   üìñ database_radion.xlsx\n",
      "      üìÑ Scopus_392: 397 studies (-1)\n",
      "      üìÑ PubMed_326: 332 studies (-4)\n",
      "      üìÑ Web_Of_Science_275: 281 studies (-3)\n",
      "      üìÑ CochraneLibrary_46: 50 studies (-2)\n",
      "      üìÑ Suplementary Search_2: 4 studies (-7)\n",
      "      üìÑ Fulltextscreening_42: 47 studies (-6)\n",
      "      üìÑ Paper_Final for Extraction_16 : 16 studies\n",
      "‚úÖ Total: 1127 studies\n",
      "üîç Checking 5 new vs 1127 existing...\n",
      "   üìä 10224 searchable values\n",
      "   üîÑ PMID: 40671095...\n",
      "   üîÑ PMID: 40669117...\n",
      "   üîÑ PMID: 40379592...\n",
      "   üîÑ PMID: 40079925...\n",
      "üìä 4 duplicates found\n",
      "\n",
      "‚úÖ Summary: 5 retrieved, 4 removed, 1 unique\n",
      "\n",
      "üéâ Deduplication completed successfully!\n",
      "   üì• Original studies: 5\n",
      "   üîÑ Duplicates removed: 4\n",
      "   ‚ú® Unique studies remaining: 1\n",
      "   üìä Deduplication rate: 80.0%\n",
      "\n",
      "üìÑ Sample deduplicated studies:\n",
      "   1. [2025] Comparing the Perioperative and Oncological Outcomes of Open Versus Mi...\n",
      "\n",
      "üí° Next step: Export results or proceed to AI screening\n"
     ]
    }
   ],
   "source": [
    "# Execute deduplication\n",
    "print(\"üßπ Starting deduplication process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have studies to deduplicate\n",
    "if studies_df.empty:\n",
    "    print(\"‚ùå No studies available for deduplication\")\n",
    "    print(\"   Please run the PubMed search first\")\n",
    "    deduplicated_df = pd.DataFrame()\n",
    "    dedup_stats = {'total_new': 0, 'duplicates_found': 0, 'unique_studies': 0, 'database_files': []}\n",
    "else:\n",
    "    # Perform deduplication\n",
    "    deduplicated_df, dedup_stats = deduplicate_studies(studies_df, database_folder=\"old_database\")\n",
    "    \n",
    "    # Store results globally for later use\n",
    "    DEDUPLICATION_STATS = dedup_stats\n",
    "    \n",
    "    if not deduplicated_df.empty:\n",
    "        print(f\"\\nüéâ Deduplication completed successfully!\")\n",
    "        print(f\"   üì• Original studies: {dedup_stats['total_new']}\")\n",
    "        print(f\"   üîÑ Duplicates removed: {dedup_stats['duplicates_found']}\")\n",
    "        print(f\"   ‚ú® Unique studies remaining: {dedup_stats['unique_studies']}\")\n",
    "        print(f\"   üìä Deduplication rate: {(dedup_stats['duplicates_found']/dedup_stats['total_new']*100):.1f}%\")\n",
    "        \n",
    "        # Show sample of deduplicated studies\n",
    "        print(f\"\\nüìÑ Sample deduplicated studies:\")\n",
    "        for i, (_, row) in enumerate(deduplicated_df.head(3).iterrows()):\n",
    "            title = row.get('Title', 'No title')\n",
    "            year = row.get('Publication Year', 'N/A')\n",
    "            print(f\"   {i+1}. [{year}] {title[:70]}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  All studies were duplicates - no unique studies remaining\")\n",
    "\n",
    "print(f\"\\nüí° Next step: Export results or proceed to AI screening\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0082a80",
   "metadata": {},
   "source": [
    "## üéØ Step 11: Extraction of Inclusion/Exclusion Criteria\n",
    "\n",
    "**AI Analysis**: Extract screening criteria from protocol using the most intelligent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178cd46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Criteria extraction function ready!\n",
      "   Function: extract_screening_criteria_from_protocol()\n"
     ]
    }
   ],
   "source": [
    "def extract_screening_criteria_from_protocol(protocol_file='protocol.docx', cache_file=None):\n",
    "    \"\"\"Extract inclusion and exclusion criteria from protocol using AI.\"\"\"\n",
    "    \n",
    "    # Use session cache directory if not specified\n",
    "    if cache_file is None:\n",
    "        cache_file = f\"{CACHE_DIR}/screening_criteria.json\"\n",
    "    \n",
    "    # Check if we have cached criteria\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"‚úÖ Loading cached criteria from {cache_file}\")\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            criteria = json.load(f)\n",
    "        print(f\"   ‚úì Inclusion criteria: {len(criteria['inclusion_criteria'])} items\")\n",
    "        print(f\"   ‚úì Exclusion criteria: {len(criteria['exclusion_criteria'])} items\")\n",
    "        return criteria\n",
    "    \n",
    "    # Extract protocol text\n",
    "    print(f\"üìÑ Reading protocol from {protocol_file}...\")\n",
    "    protocol_text = extract_text_from_protocol(protocol_file)\n",
    "    \n",
    "    if not protocol_text:\n",
    "        print(\"‚ùå Could not read protocol file\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Protocol loaded: {len(protocol_text)} characters\")\n",
    "    \n",
    "    # AI prompt for criteria extraction\n",
    "    prompt = f\"\"\"You are an expert systematic review methodologist. \n",
    "\n",
    "TASK: Carefully analyze the protocol below and extract ALL inclusion and exclusion criteria for abstract screening.\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Extract EVERY inclusion criterion mentioned in the protocol\n",
    "2. Extract EVERY exclusion criterion mentioned in the protocol\n",
    "3. Be specific and clear - each criterion should be actionable for screening\n",
    "4. Maintain the original intent and specificity from the protocol\n",
    "5. Include criteria about: study design, population, intervention, outcomes, language, publication type, etc.\n",
    "\n",
    "OUTPUT FORMAT (mandatory - valid JSON only):\n",
    "{{\n",
    "  \"inclusion_criteria\": [\n",
    "    \"criterion 1\",\n",
    "    \"criterion 2\",\n",
    "    ...\n",
    "  ],\n",
    "  \"exclusion_criteria\": [\n",
    "    \"criterion 1\",\n",
    "    \"criterion 2\",\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "IMPORTANT: \n",
    "- Output ONLY valid JSON, nothing else\n",
    "- Be comprehensive - include all relevant criteria\n",
    "- Use clear, specific language\n",
    "\n",
    "---\n",
    "\n",
    "PROTOCOL:\n",
    "{protocol_text}\n",
    "\n",
    "Extract the criteria now (JSON only):\"\"\"\n",
    "\n",
    "    print(\"üß† Analyzing protocol with AI (using query_model)...\")\n",
    "    response = call_llm(\n",
    "        prompt=prompt,\n",
    "        model=LLM_CONFIG['query_model'],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    if not response:\n",
    "        print(\"‚ùå Failed to get AI response\")\n",
    "        return None\n",
    "    \n",
    "    # Extract JSON from response\n",
    "    try:\n",
    "        # Try to find JSON in response\n",
    "        json_match = re.search(r'\\{[\\s\\S]*\\}', response)\n",
    "        if json_match:\n",
    "            criteria = json.loads(json_match.group())\n",
    "        else:\n",
    "            criteria = json.loads(response)\n",
    "        \n",
    "        # Validate structure\n",
    "        if 'inclusion_criteria' not in criteria or 'exclusion_criteria' not in criteria:\n",
    "            print(\"‚ùå Invalid criteria structure\")\n",
    "            return None\n",
    "        \n",
    "        # Save to cache\n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(criteria, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Criteria extracted and saved to {cache_file}\")\n",
    "        print(f\"   ‚úì Inclusion criteria: {len(criteria['inclusion_criteria'])} items\")\n",
    "        print(f\"   ‚úì Exclusion criteria: {len(criteria['exclusion_criteria'])} items\")\n",
    "        \n",
    "        return criteria\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Failed to parse JSON response: {e}\")\n",
    "        print(f\"Response was: {response[:500]}...\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Criteria extraction function ready!\")\n",
    "print(\"   Function: extract_screening_criteria_from_protocol()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a13c5",
   "metadata": {},
   "source": [
    "## üìã Step 12: Execute Criteria Extraction\n",
    "\n",
    "Extract and cache screening criteria from the protocol document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de352fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading cached criteria from cache/session_20251016_232310/screening_criteria.json\n",
      "   ‚úì Inclusion criteria: 6 items\n",
      "   ‚úì Exclusion criteria: 2 items\n",
      "\n",
      "üìã INCLUSION (6 criteria):\n",
      "  1. Study population consists of patients diagnosed with penile cancer who have undergone lymphadenectomy.\n",
      "  2. Study assesses one or more clinicopathological factors (e.g., lymphovascular invasion, extranodal extension, tumor grade, perineural invasion, nodal involvement) in relation to recurrence.\n",
      "  3. Study reports on recurrence outcomes, including local, regional, or distant recurrence, or time to recurrence.\n",
      "  4. Study design is an observational study (cohort, case-control, or cross-sectional) or a clinical trial.\n",
      "  5. Study is published in a peer-reviewed journal.\n",
      "  6. Study is published in any language, provided an English translation is available.\n",
      "\n",
      "üö´ EXCLUSION (2 criteria):\n",
      "  1. Publication type is a case report, review, editorial, or conference abstract.\n",
      "  2. Study provides insufficient data on recurrence outcomes.\n",
      "\n",
      "‚úÖ Ready for screening!\n"
     ]
    }
   ],
   "source": [
    "# Extract screening criteria\n",
    "SCREENING_CRITERIA = extract_screening_criteria_from_protocol()\n",
    "\n",
    "if SCREENING_CRITERIA:\n",
    "    print(f\"\\nüìã INCLUSION ({len(SCREENING_CRITERIA['inclusion_criteria'])} criteria):\")\n",
    "    for i, c in enumerate(SCREENING_CRITERIA['inclusion_criteria'], 1):\n",
    "        print(f\"  {i}. {c}\")\n",
    "    \n",
    "    print(f\"\\nüö´ EXCLUSION ({len(SCREENING_CRITERIA['exclusion_criteria'])} criteria):\")\n",
    "    for i, c in enumerate(SCREENING_CRITERIA['exclusion_criteria'], 1):\n",
    "        print(f\"  {i}. {c}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Ready for screening!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed - check protocol.docx and API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a427462",
   "metadata": {},
   "source": [
    "## üî¨ Step 13: AI Abstract Screening Function\n",
    "\n",
    "Screen studies against inclusion/exclusion criteria using the standard LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9887a07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Screening function ready!\n"
     ]
    }
   ],
   "source": [
    "def screen_studies(studies_df, criteria, delay=1.0):\n",
    "    \"\"\"Screen all studies using AI against inclusion/exclusion criteria.\"\"\"\n",
    "    \n",
    "    print(f\"üî¨ Screening {len(studies_df)} studies (~{len(studies_df)*delay/60:.1f} min)\")\n",
    "    \n",
    "    # Build criteria lists once\n",
    "    inc_list = '\\n'.join(f\"{i+1}. {c}\" for i, c in enumerate(criteria['inclusion_criteria']))\n",
    "    exc_list = '\\n'.join(f\"{i+1}. {c}\" for i, c in enumerate(criteria['exclusion_criteria']))\n",
    "    \n",
    "    results = []\n",
    "    for idx, (_, study) in enumerate(studies_df.iterrows(), 1):\n",
    "        print(f\"[{idx}/{len(studies_df)}] {study.get('Title', 'No title')[:60]}...\")\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"You are a systematic review expert performing abstract screening.\n",
    "\n",
    "Your task: Evaluate if this study meets the inclusion criteria or should be excluded.\n",
    "\n",
    "INCLUSION CRITERIA (must meet ALL):\n",
    "{inc_list}\n",
    "\n",
    "EXCLUSION CRITERIA (exclude if meets ANY):\n",
    "{exc_list}\n",
    "\n",
    "STUDY TO EVALUATE:\n",
    "Title: {study.get('Title', 'No title')}\n",
    "Year: {study.get('Publication Year', 'N/A')}\n",
    "Abstract: {study.get('Abstract', 'No abstract')}\n",
    "\n",
    "Instructions:\n",
    "1. Read the abstract carefully\n",
    "2. Check if the study meets ALL inclusion criteria\n",
    "3. Check if the study meets ANY exclusion criteria\n",
    "4. Make your decision: INCLUDE only if it meets all inclusion criteria AND no exclusion criteria\n",
    "\n",
    "Output format (mandatory):\n",
    "DECISION: [INCLUDE or EXCLUDE]\n",
    "REASON: [Brief explanation citing specific criteria]\"\"\"\n",
    "\n",
    "        # Call AI\n",
    "        response = call_llm(prompt, temperature=0.1)\n",
    "        \n",
    "        # Parse response\n",
    "        if response:\n",
    "            decision_match = re.search(r'DECISION:\\s*(INCLUDE|EXCLUDE)', response, re.IGNORECASE)\n",
    "            reason_match = re.search(r'REASON:\\s*(.+?)(?:\\n|$)', response, re.IGNORECASE | re.DOTALL)\n",
    "            decision = decision_match.group(1).upper() if decision_match else 'UNCLEAR'\n",
    "            reason = (reason_match.group(1).strip() if reason_match else 'No reason')[:500]\n",
    "        else:\n",
    "            decision, reason = 'ERROR', 'No AI response'\n",
    "        \n",
    "        # Store result with study data\n",
    "        results.append({\n",
    "            'decision': decision,\n",
    "            'reason': reason,\n",
    "            'PMID': study.get('PMID', ''),\n",
    "            'Title': study.get('Title', ''),\n",
    "            'Authors': study.get('Authors', ''),\n",
    "            'Publication Year': study.get('Publication Year', ''),\n",
    "            'Journal/Book': study.get('Journal/Book', ''),\n",
    "            'Abstract': study.get('Abstract', ''),\n",
    "            'DOI': study.get('DOI', ''),\n",
    "            'Citation': study.get('Citation', '')\n",
    "        })\n",
    "        \n",
    "        emoji = \"‚úÖ\" if decision == 'INCLUDE' else \"‚ùå\" if decision == 'EXCLUDE' else \"‚ö†Ô∏è\"\n",
    "        print(f\"{emoji} {decision}: {reason[:80]}...\")\n",
    "        \n",
    "        if idx < len(studies_df):\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"‚úÖ Screening function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326eb55",
   "metadata": {},
   "source": [
    "## üöÄ Step 14: Execute Screening\n",
    "\n",
    "Screen all deduplicated studies using AI and the extracted criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3363e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Starting AI-powered abstract screening...\n",
      "======================================================================\n",
      "üî¨ Screening 1 studies (~0.0 min)\n",
      "[1/1] Comparing the Perioperative and Oncological Outcomes of Open...\n",
      "‚ùå EXCLUDE: The study is a systematic review and meta-analysis, which is an exclusion criter...\n",
      "\n",
      "======================================================================\n",
      "üìä SCREENING SUMMARY\n",
      "======================================================================\n",
      "Total studies screened: 1\n",
      "‚úÖ INCLUDED: 0 (0.0%)\n",
      "‚ùå EXCLUDED: 1 (100.0%)\n",
      "\n",
      "‚úÖ Screening completed!\n",
      "üí° Next step: Export results\n"
     ]
    }
   ],
   "source": [
    "# Execute screening\n",
    "print(\"üî¨ Starting AI-powered abstract screening...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify we have everything we need\n",
    "if deduplicated_df.empty:\n",
    "    print(\"‚ùå No deduplicated studies available\")\n",
    "    print(\"   Please run the deduplication step first\")\n",
    "    screening_results_df = pd.DataFrame()\n",
    "elif not SCREENING_CRITERIA:\n",
    "    print(\"‚ùå No screening criteria available\")\n",
    "    print(\"   Please run the criteria extraction step first\")\n",
    "    screening_results_df = pd.DataFrame()\n",
    "else:\n",
    "    # Perform screening\n",
    "    screening_results_df = screen_studies(\n",
    "        studies_df=deduplicated_df,\n",
    "        criteria=SCREENING_CRITERIA,\n",
    "        delay=1.0\n",
    "    )\n",
    "    \n",
    "    # Display summary\n",
    "    if not screening_results_df.empty:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"üìä SCREENING SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        total = len(screening_results_df)\n",
    "        included = len(screening_results_df[screening_results_df['decision'] == 'INCLUDE'])\n",
    "        excluded = len(screening_results_df[screening_results_df['decision'] == 'EXCLUDE'])\n",
    "        unclear = len(screening_results_df[screening_results_df['decision'] == 'UNCLEAR'])\n",
    "        errors = len(screening_results_df[screening_results_df['decision'] == 'ERROR'])\n",
    "        \n",
    "        print(f\"Total studies screened: {total}\")\n",
    "        print(f\"‚úÖ INCLUDED: {included} ({included/total*100:.1f}%)\")\n",
    "        print(f\"‚ùå EXCLUDED: {excluded} ({excluded/total*100:.1f}%)\")\n",
    "        if unclear > 0:\n",
    "            print(f\"‚ö†Ô∏è  UNCLEAR: {unclear} ({unclear/total*100:.1f}%)\")\n",
    "        if errors > 0:\n",
    "            print(f\"üî¥ ERRORS: {errors} ({errors/total*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample of included studies\n",
    "        included_studies = screening_results_df[screening_results_df['decision'] == 'INCLUDE']\n",
    "        if not included_studies.empty:\n",
    "            print(f\"\\nüìÑ Sample of INCLUDED studies:\")\n",
    "            for i, (_, row) in enumerate(included_studies.head(3).iterrows(), 1):\n",
    "                print(f\"{i}. {row['Title'][:70]}...\")\n",
    "                print(f\"   Reason: {row['reason'][:100]}...\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Screening completed!\")\n",
    "        print(\"üí° Next step: Export results\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No screening results generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c5e6a",
   "metadata": {},
   "source": [
    "## üìä Step 15: Export Results\n",
    "\n",
    "Save all results to organized session folder with timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af0aa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Exporting results...\n",
      "üìÅ Exporting to: export_results/session_20251016_232310/\n",
      "‚úÖ Query saved: export_results/session_20251016_232310/search_query.txt\n",
      "‚úÖ Criteria saved: export_results/session_20251016_232310/screening_criteria.json\n",
      "‚úÖ Deduplicated studies saved: export_results/session_20251016_232310/deduplicated_studies.xlsx\n",
      "   (1 studies)\n",
      "‚úÖ Screening results saved: export_results/session_20251016_232310/screening_results.xlsx\n",
      "   (0 included, 1 excluded)\n",
      "\n",
      "üéâ Export completed!\n",
      "üìä Results: export_results/session_20251016_232310/\n",
      "üíæ Cache: cache/session_20251016_232310/\n"
     ]
    }
   ],
   "source": [
    "# Export all results to session folder\n",
    "print(\"üìä Exporting results...\")\n",
    "\n",
    "# All exports go in the export directory\n",
    "print(f\"üìÅ Exporting to: {EXPORT_DIR}/\")\n",
    "\n",
    "# 1. Export PubMed search query (TXT)\n",
    "if SEARCH_QUERY:\n",
    "    query_file = f\"{EXPORT_DIR}/search_query.txt\"\n",
    "    with open(query_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"PubMed Search Query\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"{'=' * 70}\\n\\n\")\n",
    "        f.write(SEARCH_QUERY)\n",
    "    print(f\"‚úÖ Query saved: {query_file}\")\n",
    "\n",
    "# 2. Export screening criteria (JSON)\n",
    "if SCREENING_CRITERIA:\n",
    "    criteria_file = f\"{EXPORT_DIR}/screening_criteria.json\"\n",
    "    with open(criteria_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(SCREENING_CRITERIA, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Criteria saved: {criteria_file}\")\n",
    "\n",
    "# 3. Export deduplicated studies (XLSX)\n",
    "if not deduplicated_df.empty:\n",
    "    dedup_file = f\"{EXPORT_DIR}/deduplicated_studies.xlsx\"\n",
    "    deduplicated_df.to_excel(dedup_file, index=False)\n",
    "    print(f\"‚úÖ Deduplicated studies saved: {dedup_file}\")\n",
    "    print(f\"   ({len(deduplicated_df)} studies)\")\n",
    "\n",
    "# 4. Export screening results (XLSX)\n",
    "if not screening_results_df.empty:\n",
    "    screening_file = f\"{EXPORT_DIR}/screening_results.xlsx\"\n",
    "    screening_results_df.to_excel(screening_file, index=False)\n",
    "    print(f\"‚úÖ Screening results saved: {screening_file}\")\n",
    "    \n",
    "    included = len(screening_results_df[screening_results_df['decision'] == 'INCLUDE'])\n",
    "    excluded = len(screening_results_df[screening_results_df['decision'] == 'EXCLUDE'])\n",
    "    print(f\"   ({included} included, {excluded} excluded)\")\n",
    "\n",
    "print(f\"\\nüéâ Export completed!\")\n",
    "print(f\"üìä Results: {EXPORT_DIR}/\")\n",
    "print(f\"üíæ Cache: {CACHE_DIR}/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
